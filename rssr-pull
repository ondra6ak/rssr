#! /usr/bin/python3

import requests
import rssr
import argparse
from os import chdir, environ
from time import strftime

parser = argparse.ArgumentParser()
parser.add_argument("-f", "--feeds", dest="feeds", metavar="FEEDS", help="file with individual urls", default=environ["HOME"] + "/.rssr/feeds")
parser.add_argument("-q", "--quiet", dest="verbose", help="run in quiet mode", action="store_false", default=True)
parser.add_argument("directory", nargs="?", metavar="DIR", help="directory to wich articles will be downloaded", default=environ["HOME"] + "/.rssr/articles/")
args = parser.parse_args()

chdir(args.directory)

webs = rssr.feeds_get(open(args.feeds, "r").readlines())

for web in webs:
    for entry in web["entries"]:
        try: published = strftime("%F %H:%M", entry["published_parsed"])
        except KeyError: published = strftime("%F %H:%M", entry["updated_parsed"])

        artf = open("{} {}: {}".format(published, web["feed"]["title"], entry["title"]).replace("/", ""), "w")
        
        if args.verbose: print("{} {}: {}".format(published, web["feed"]["title"], entry["title"]).replace("/", ""))
        
        artr = requests.get(entry["links"][0]["href"])
        if not artr.ok:
            raise Exception("Not OK response {}".format(artr.url))
        
        html = rssr.html2text(artr.content).decode(artr.encoding)
        beg = html.find("<h1")
        if beg < 0: beg = 0
        artf.write("<html>\n<meta charset=utf-8>\n" + html[beg:] + "</html>")
        artf.close()
