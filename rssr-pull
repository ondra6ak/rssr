#! /usr/bin/python3

import requests
import rssr
import feedparser
import argparse
from os import chdir, environ
from time import strftime

parser = argparse.ArgumentParser()
parser.add_argument("-f", "--feeds", dest="feeds", metavar="FEEDS", help="file with individual urls", default=environ["HOME"] + "/.rssr/feeds")
parser.add_argument("-q", "--quiet", dest="verbose", help="run in quiet mode", action="store_false", default=True)
parser.add_argument("directory", nargs="?", metavar="DIR", help="directory to wich articles will be downloaded", default=environ["HOME"] + "/.rssr/articles/")
args = parser.parse_args()

chdir(args.directory)

for url in open(args.feeds, "r").readlines():
    feed = feedparser.parse(url)

    for entry in feed["entries"]:
        try: published = strftime("%F %H:%M", entry["published_parsed"])
        except KeyError: published = strftime("%F %H:%M", entry["updated_parsed"])
        
        filename = "{} {}: {}".format(published, feed["feed"]["title"], entry["title"]).replace("/", "")
        artf = open(filename, "w")
        if args.verbose: print(filename)
        
        artr = requests.get(entry["link"])
        if not artr.ok:
            raise Exception("Not OK response {}".format(artr.url))
        
        html = rssr.html2content(artr.content).decode(artr.encoding)
        beg = html.find("<h1")
        if beg < 0: beg = 0
        artf.write("<html>\n<meta charset=utf-8>\n" + html[beg:] + "</html>")
        artf.close()
